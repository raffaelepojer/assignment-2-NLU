{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "088be119-c8b9-4409-b33e-b95586b051f3",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "\n",
    "Assigment is in the intersection of Named Entity Recognition and Dependency Parsing.\n",
    "\n",
    "0. Evaluate spaCy NER on CoNLL 2003 data (provided)\n",
    "    - report token-level performance (per class and total)\n",
    "        - accuracy of correctly recognizing all tokens that belong to named entities (i.e. tag-level accuracy) \n",
    "    - report CoNLL chunk-level performance (per class and total);\n",
    "        - precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total  \n",
    "\n",
    "1. Grouping of Entities.\n",
    "Write a function to group recognized named entities using `noun_chunks` method of [spaCy](https://spacy.io/usage/linguistic-features#noun-chunks). Analyze the groups in terms of most frequent combinations (i.e. NER types that go together). \n",
    "\n",
    "2. One of the possible post-processing steps is to fix segmentation errors.\n",
    "Write a function that extends the entity span to cover the full noun-compounds. Make use of `compound` dependency relation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31406b-1d2f-4c1d-8eac-263c397ae332",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Import all the python libraries and read the dataset file. Here there are the functions to read the conll txt file. Also, I've removed all the _-DOCSTART-_ from the dataset.\n",
    "The two main functions are:\n",
    "- __create_corpus(data)__: this function reads the file and returns a list of list. Each list contain the string of the sentence just read from the dataset\n",
    "\n",
    "ex:\n",
    "```\n",
    "[[\"SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .\"], ...]\n",
    "```\n",
    "- __create_corpus_ner(data)__: this function reads the file and returns a list of list. Each list contain the tuple _(Token, name_entity)_\n",
    "\n",
    "ex:\n",
    "```\n",
    "[[('SOCCER', 'O'), ('-', 'O'), ('JAPAN', 'B-LOC'), ('GET', 'O'), ('LUCKY', 'O'), ('WIN', 'O'), (',', 'O'), ('CHINA', 'B-PER'), ('IN', 'O'), ('SURPRISE', 'O'), ('DEFEAT', 'O'), ('.', 'O')], ...]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "__NOTE:__\n",
    "It would be more efficient to process all the sentence and save the spacy doc in an external list, but for clarity, each function is re-processed by SpaCy nlp.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14fb5388-4b81-45f7-a50b-b3e3f8752fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .\n",
      "\n",
      "[('SOCCER', 'O'), ('-', 'O'), ('JAPAN', 'B-LOC'), ('GET', 'O'), ('LUCKY', 'O'), ('WIN', 'O'), (',', 'O'), ('CHINA', 'B-PER'), ('IN', 'O'), ('SURPRISE', 'O'), ('DEFEAT', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp\n",
    "import conll\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "corpus_data = conll.read_corpus_conll(\"data/conll2003/train.txt\")\n",
    "test_data = conll.read_corpus_conll(\"data/conll2003/test.txt\")\n",
    "\n",
    "def read_data_fields(corpus_file):\n",
    "    return [[data[0].split() for data in sent] for sent in corpus_file]\n",
    "\n",
    "def get_data_text(data):\n",
    "    return [sent[0] for sent in data]\n",
    "\n",
    "def get_data_ner(data):\n",
    "    return [(sent[0], sent[3]) for sent in data]\n",
    "\n",
    "def create_corpus(read_corpus):\n",
    "    text = []\n",
    "    for sent in read_corpus:\n",
    "        tmp = get_data_text(sent)\n",
    "        if tmp != [\"-DOCSTART-\"]:\n",
    "            text.append(\" \".join(tmp))\n",
    "    return text\n",
    "\n",
    "def create_corpus_ner(read_corpus):\n",
    "    ner_corpus = []\n",
    "    for sent in read_corpus:\n",
    "        tmp = get_data_ner(sent)\n",
    "        if tmp[0][0] != \"-DOCSTART-\":\n",
    "            ner_corpus.append(tmp)\n",
    "    return ner_corpus\n",
    "\n",
    "#################################################\n",
    "\n",
    "# Read the datest from conll2003/test.txt\n",
    "\n",
    "data = read_data_fields(test_data)\n",
    "corpus_text = create_corpus(data)\n",
    "corpus_ner = create_corpus_ner(data)\n",
    "\n",
    "# Output example of create_corpus and corpus_corpus_ner\n",
    "\n",
    "print(corpus_text[0])\n",
    "print()\n",
    "print(corpus_ner[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b5b2ed-5140-468f-84eb-0a263a531cb0",
   "metadata": {},
   "source": [
    "## Point 0.1\n",
    "- report token-level performance (per class and total)\n",
    "    - accuracy of correctly recognizing all tokens that belong to named entities (i.e. tag-level accuracy) \n",
    "\n",
    "---\n",
    "\n",
    "To report the token-level performance, the first thing to do is to convert the spacy token in a conll format. The entity types of spacy that do not belong to the conll format were linked to the _MISC_ type. Each sentence is processed by SpaCy nlp, and then post-processed to match the conll.\n",
    "\n",
    "The function __spacy_token(doc)__ take as input a spacy doc and returns a new list with the same characteristic as the output of __create_corpus_ner__. It makes use of the _token.whitespace__ to detect when a token is read.\n",
    "\n",
    "To convert the spacy entity type the function __convert_ent_type(ent_type)__ use a dictionay to return the new type as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f8ad73-4f60-4a12-a1e3-6c72e4ffc49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AL-AIN', 'B-ORG'), (',', 'O'), ('United', 'B-ORG'), ('Arab', 'I-ORG'), ('Emirates', 'I-ORG'), ('1996-12-06', 'B-MISC')]\n"
     ]
    }
   ],
   "source": [
    "def convert_ent_type(ent_type):\n",
    "    ent = {\n",
    "    'PERSON': 'PER',\n",
    "    'NORP': 'MISC',\n",
    "    'FAC': 'LOC',\n",
    "    'ORG': 'ORG',\n",
    "    'GPE': 'LOC',\n",
    "    'LOC': 'LOC',\n",
    "    'PRODUCT': 'MISC',\n",
    "    'EVENT': 'MISC',\n",
    "    'WORK_OF_ART': 'MISC',\n",
    "    'LAW': 'MISC',\n",
    "    'LANGUAGE': 'MISC',\n",
    "    'DATE': 'MISC',\n",
    "    'TIME': 'MISC',\n",
    "    'PERCENT': 'MISC',\n",
    "    'MONEY': 'MISC',\n",
    "    'QUANTITY': 'MISC',\n",
    "    'ORDINAL': 'MISC',\n",
    "    'CARDINAL': 'MISC',\n",
    "    '': ''}\n",
    "    return ent[ent_type]\n",
    "\n",
    "def spacy_token(doc):\n",
    "    token_list = []\n",
    "    string = \"\"\n",
    "    iob = \"\"\n",
    "    first = True\n",
    "    for ind, token in enumerate(doc):\n",
    "        string += token.text\n",
    "        # save only the fist iob of a ner\n",
    "        if first:\n",
    "            if token.ent_iob_ == \"O\": # O type\n",
    "                iob = token.ent_iob_\n",
    "            else:\n",
    "                iob = token.ent_iob_ + \"-\" + convert_ent_type(token.ent_type_) # B-TYPE or I-TYPE\n",
    "            first = False\n",
    "\n",
    "        # when a space is found the ner is complete\n",
    "        if token.whitespace_ == \" \":\n",
    "            token_list.append((string, iob))\n",
    "            string = \"\"\n",
    "            first = True\n",
    "\n",
    "        # add last token\n",
    "        if ind == len(doc)-1:\n",
    "            token_list.append((string, iob))\n",
    "\n",
    "    return token_list\n",
    "\n",
    "#################################################\n",
    "\n",
    "# Output example of spacy_token\n",
    "\n",
    "print(spacy_token(nlp(corpus_text[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37afe259-3659-44ab-a291-8c45e8d9ecf5",
   "metadata": {},
   "source": [
    "To report the accuracy-level, I go through the entire sentences of the dataset, process it with _nlp_ and then save all the spacy labels and conll labels in two lists. The report is then computed by the function of scikit-learn _classification_report_ in __accuracy_token_level(data)__, the output is the report given by scikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dbb3a32-7530-439f-b62a-ec4bab666a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.76      0.68      0.72      1668\n",
      "      B-MISC       0.10      0.57      0.17       702\n",
      "       B-ORG       0.52      0.31      0.38      1661\n",
      "       B-PER       0.80      0.63      0.70      1617\n",
      "       I-LOC       0.54      0.56      0.55       257\n",
      "      I-MISC       0.05      0.38      0.09       216\n",
      "       I-ORG       0.42      0.51      0.46       835\n",
      "       I-PER       0.84      0.79      0.81      1156\n",
      "           O       0.94      0.86      0.90     38323\n",
      "\n",
      "    accuracy                           0.81     46435\n",
      "   macro avg       0.55      0.59      0.53     46435\n",
      "weighted avg       0.89      0.81      0.84     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def accuracy_token_level(data):\n",
    "    corpus_ner = create_corpus_ner(data)\n",
    "    corpus_text = create_corpus(data)\n",
    "    list_spacy = []\n",
    "    list_total = []\n",
    "\n",
    "    for index, sent in enumerate(corpus_ner):\n",
    "        spacy_doc = nlp(corpus_text[index])\n",
    "        tokens = spacy_token(spacy_doc)\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            list_total.append(sent[i][1])\n",
    "            list_spacy.append(tokens[i][1])\n",
    "\n",
    "    return classification_report(list_total, list_spacy)\n",
    "\n",
    "print(accuracy_token_level(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92301a5-c23c-48d0-b47e-b1a5833e888b",
   "metadata": {},
   "source": [
    "## Point 0.2\n",
    "\n",
    "- report CoNLL chunk-level performance (per class and total);\n",
    "    - precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total\n",
    "    \n",
    "---\n",
    "\n",
    "The chunk-level accuracy is computed by the __conll.evaluate__ function. Here it is a little bit simpler: whit __accuracy_chunk_level(data)__ I append in a list (_refs_) all the converted spacy tokens, and compare them with the dataset tokens inside the _corpus_ner_ list. The results are then stored and showed in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ef3414-30d7-412c-beed-e27268692721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.672</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.708</td>\n",
       "      <td>1499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.553</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.169</td>\n",
       "      <td>3879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.276</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.346</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.609</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.681</td>\n",
       "      <td>1271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.523</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.444</td>\n",
       "      <td>7638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           p      r      f     s\n",
       "LOC    0.672  0.748  0.708  1499\n",
       "MISC   0.553  0.100  0.169  3879\n",
       "ORG    0.276  0.464  0.346   989\n",
       "PER    0.609  0.774  0.681  1271\n",
       "total  0.523  0.386  0.444  7638"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy_chunk_level(data):\n",
    "    corpus_ner = create_corpus_ner(data)\n",
    "    corpus_text = create_corpus(data)\n",
    "    hyps = corpus_ner\n",
    "    refs = []\n",
    "\n",
    "    for index in range(len(corpus_text)):\n",
    "        spacy_doc = nlp(corpus_text[index])\n",
    "        tokens = spacy_token(spacy_doc)\n",
    "        refs.append(tokens)\n",
    "\n",
    "    results = conll.evaluate(refs, hyps)\n",
    "    pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "    return pd_tbl.round(decimals=3)\n",
    "\n",
    "accuracy_chunk_level(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19cab2a-842d-4eef-a85f-b1436ecadb06",
   "metadata": {},
   "source": [
    "## Point 1. Grouping of Entities\n",
    "Write a function to group recognized named entities using `noun_chunks` method of [spaCy](https://spacy.io/usage/linguistic-features#noun-chunks). Analyze the groups in terms of most frequent combinations (i.e. NER types that go together).\n",
    "\n",
    "---\n",
    "\n",
    "The main function is __gropu_ner(data)__. This function takes as input a string of a sentence and returns a list of lists containing all the entities. To include all the single tokens which do not are in the chunk, a second for loop iterate all over the entities of the doc, excluding the one that is already in the noun_chunks.\n",
    "To then count the frequency of each group recognized, the function __fr_comb(data)__ takes as input a list of sentences, process the sentences with __group_ner__ and then save each group in a dictionary, where the keys of this dictionary are the groups converted in string by the function __key_string(chunk)__.\n",
    "\n",
    "I decided to keep the order of the entities distinct to preserve the \"syntactic\" meaning. The output is the ordered dictionary by the number of frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81808a3e-6670-4a91-aa76-1b1cd5c77e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CARDINAL] : 1624\n",
      "[GPE] : 1255\n",
      "[PERSON] : 1074\n",
      "[DATE] : 997\n",
      "[ORG] : 873\n",
      "[NORP] : 293\n",
      "[MONEY] : 147\n",
      "[ORDINAL] : 111\n",
      "[TIME] : 83\n",
      "[PERCENT] : 81\n",
      "[EVENT] : 58\n",
      "[LOC] : 54\n",
      "[CARDINAL, PERSON] : 51\n",
      "[QUANTITY] : 51\n",
      "[NORP, PERSON] : 43\n",
      "[GPE, PERSON] : 34\n",
      "[GPE, GPE] : 26\n",
      "[FAC] : 22\n",
      "[PRODUCT] : 22\n",
      "[ORG, PERSON] : 21\n",
      "[CARDINAL, ORG] : 19\n",
      "[CARDINAL, NORP] : 15\n",
      "[CARDINAL, GPE] : 13\n",
      "[GPE, ORG] : 13\n",
      "[LAW] : 11\n",
      "[WORK_OF_ART] : 10\n",
      "[GPE, PRODUCT] : 9\n",
      "[DATE, EVENT] : 8\n",
      "[DATE, ORG] : 8\n",
      "[ORG, ORG] : 8\n",
      "[PERSON, PERSON] : 8\n",
      "[NORP, ORG] : 8\n",
      "[DATE, TIME] : 7\n",
      "[ORG, DATE] : 6\n",
      "[LANGUAGE] : 6\n",
      "[GPE, DATE] : 5\n",
      "[CARDINAL, CARDINAL] : 5\n",
      "[NORP, ORDINAL] : 5\n",
      "[ORG, GPE] : 5\n",
      "[DATE, NORP] : 5\n",
      "[GPE, ORDINAL] : 4\n",
      "[ORDINAL, PERSON] : 4\n",
      "[GPE, CARDINAL] : 4\n",
      "[ORG, NORP] : 4\n",
      "[PERSON, GPE] : 4\n",
      "[CARDINAL, DATE] : 3\n",
      "[ORG, CARDINAL] : 3\n",
      "[CARDINAL, PERSON, CARDINAL] : 3\n",
      "[NORP, NORP] : 3\n",
      "[PERSON, PERSON, PERSON] : 2\n",
      "[CARDINAL, ORDINAL] : 2\n",
      "[CARDINAL, CARDINAL, PERSON] : 2\n",
      "[ORG, ORDINAL] : 2\n",
      "[LANGUAGE, ORDINAL] : 2\n",
      "[GPE, DATE, ORG] : 2\n",
      "[ORDINAL, EVENT] : 2\n",
      "[GPE, LOC] : 2\n",
      "[CARDINAL, CARDINAL, ORG] : 2\n",
      "[QUANTITY, QUANTITY] : 2\n",
      "[GPE, NORP] : 2\n",
      "[DATE, CARDINAL] : 2\n",
      "[DATE, NORP, PERSON] : 2\n",
      "[EVENT, CARDINAL] : 2\n",
      "[GPE, FAC] : 2\n",
      "[PERSON, CARDINAL] : 2\n",
      "[ORDINAL, NORP] : 1\n",
      "[GPE, PERSON, CARDINAL] : 1\n",
      "[ORDINAL, DATE] : 1\n",
      "[ORG, GPE, ORDINAL] : 1\n",
      "[ORG, QUANTITY] : 1\n",
      "[CARDINAL, GPE, GPE] : 1\n",
      "[PERCENT, CARDINAL] : 1\n",
      "[NORP, DATE] : 1\n",
      "[PERSON, NORP] : 1\n",
      "[PERSON, ORG] : 1\n",
      "[LOC, DATE] : 1\n",
      "[DATE, FAC] : 1\n",
      "[CARDINAL, CARDINAL, NORP] : 1\n",
      "[NORP, PERSON, DATE] : 1\n",
      "[LOC, ORDINAL] : 1\n",
      "[PRODUCT, GPE] : 1\n",
      "[MONEY, ORG] : 1\n",
      "[NORP, LOC] : 1\n",
      "[ORDINAL, GPE] : 1\n",
      "[MONEY, DATE] : 1\n",
      "[CARDINAL, PERCENT] : 1\n",
      "[DATE, WORK_OF_ART] : 1\n",
      "[MONEY, MONEY] : 1\n",
      "[MONEY, CARDINAL, CARDINAL, ORG] : 1\n",
      "[CARDINAL, GPE, TIME] : 1\n",
      "[FAC, GPE] : 1\n",
      "[ORG, WORK_OF_ART] : 1\n",
      "[GPE, ORDINAL, PERSON] : 1\n",
      "[CARDINAL, LOC] : 1\n",
      "[PERSON, MONEY] : 1\n",
      "[MONEY, PRODUCT] : 1\n",
      "[DATE, PERSON] : 1\n",
      "[PERSON, GPE, CARDINAL] : 1\n",
      "[ORDINAL, ORG] : 1\n",
      "[PERSON, ORDINAL] : 1\n",
      "[EVENT, ORDINAL] : 1\n",
      "[PERSON, FAC] : 1\n",
      "[ORDINAL, TIME] : 1\n",
      "[DATE, LANGUAGE, ORDINAL] : 1\n",
      "[CARDINAL, EVENT] : 1\n"
     ]
    }
   ],
   "source": [
    "def group_ner(data):\n",
    "    spacy_doc = nlp(data)\n",
    "    group = []\n",
    "    exclude = []\n",
    "\n",
    "    for chunk in spacy_doc.noun_chunks:\n",
    "        ent = []\n",
    "        for e in chunk.ents:\n",
    "            ent.append(e.label_)\n",
    "            exclude.append(e)\n",
    "        if len(ent) > 0:\n",
    "            group.append(ent)\n",
    "    \n",
    "    # add the remaining name entities if they are not in noun_chunks\n",
    "    for e in spacy_doc.ents:\n",
    "        if e not in exclude:\n",
    "            if len([e.label_]) > 0:\n",
    "                group.append([e.label_])\n",
    "\n",
    "    return group\n",
    "\n",
    "def key_string(chunk):\n",
    "    keys = \"[\"\n",
    "    for i, el in enumerate(chunk):\n",
    "        if i == 0:\n",
    "            keys += el\n",
    "        else:\n",
    "            keys = \", \".join([keys, el])\n",
    "    keys += \"]\"\n",
    "    return keys\n",
    "\n",
    "def fr_comb(data):\n",
    "    dic_fr = {}\n",
    "    for sent in data:\n",
    "        ner_gr = group_ner(sent)\n",
    "        for tok in ner_gr:\n",
    "            key = key_string(tok)\n",
    "            if key not in dic_fr.keys():\n",
    "                dic_fr[key] = 0\n",
    "            dic_fr[key] += 1\n",
    "\n",
    "    return dict(sorted(dic_fr.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "for key, value in fr_comb(corpus_text).items():\n",
    "    print(\"{0} : {1}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b042e922-2333-452e-b94e-6fa9f9da741c",
   "metadata": {},
   "source": [
    "## Point 2. \n",
    "One of the possible post-processing steps is to fix segmentation errors.\n",
    "Write a function that extends the entity span to cover the full noun-compounds. Make use of `compound` dependency relation.\n",
    "\n",
    "---\n",
    "\n",
    "By looking at the Spacy [documentation](https://spacy.io/usage/linguistic-features#setting-entities), there is a handy function called __set_ents__. This function reset the entity if Spacy does not detect it and put it back in the doc. To expand the entities, I iterate over all the entities detected by spacy. Then I iterate over all the token of each entity to look up at the children at the immediate left and right.\n",
    "If one of these tokens has the \"compound\" dependency relation and is outside the entity (ent._iob_ == 'O'), I expand it to include them in the new entity. The just extended entity may, in turn, have another dependency as \"compound\". All this process is done again, inside a while loop. If there are no more expanded entities, the loop ends and return the new doc with the refreshed entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fe0be9a-c34d-4fa0-9d45-221c03c9b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_entity(data):\n",
    "    spacy_doc = nlp(data)\n",
    "    new_doc = spacy_doc\n",
    "    expand = True\n",
    "\n",
    "    while expand:\n",
    "        expand = False\n",
    "        for ent in spacy_doc.ents:\n",
    "            for tok in ent:\n",
    "                for child in tok.children:\n",
    "                    # if the compound is near the entity\n",
    "\n",
    "                    if child.dep_ == \"compound\" and child.ent_iob_ == \"O\" and ent.start-1 >= 0 and child == spacy_doc[ent.start-1]:\n",
    "\n",
    "                        fb_ent = Span(new_doc, ent.start-1, ent.end, label=ent.label_)\n",
    "                        new_doc.set_ents([fb_ent], default=\"unmodified\")\n",
    "                        expand = True\n",
    "\n",
    "                    if child.dep_ == \"compound\" and child.ent_iob_ == \"O\" and ent.end+1 < len(spacy_doc) and child == spacy_doc[ent.end+1]:\n",
    "\n",
    "                        fb_ent = Span(new_doc, ent.start, ent.end+1, label=ent.label_)\n",
    "                        new_doc.set_ents([fb_ent], default=\"unmodified\")\n",
    "                        expand = True\n",
    "\n",
    "        spacy_doc = new_doc\n",
    "\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f57c3fd-8cb3-4968-ae18-6568ce5137e6",
   "metadata": {},
   "source": [
    "To calculate the accuray of the new expanded entities, I use a similar function that use the __conll.evaluate__ method with the new doc. \n",
    "\n",
    "From the results, it can be seen that the smaller are with this new method does not improve spacy accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01973c53-41e0-462a-b248-de2655c530de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.653</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.688</td>\n",
       "      <td>1499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.551</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.169</td>\n",
       "      <td>3879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.272</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.340</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.515</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.577</td>\n",
       "      <td>1271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.489</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.415</td>\n",
       "      <td>7638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           p      r      f     s\n",
       "LOC    0.653  0.726  0.688  1499\n",
       "MISC   0.551  0.100  0.169  3879\n",
       "ORG    0.272  0.456  0.340   989\n",
       "PER    0.515  0.655  0.577  1271\n",
       "total  0.489  0.361  0.415  7638"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy_expansion(data):\n",
    "    corpus_ner = create_corpus_ner(data)\n",
    "    corpus_text = create_corpus(data)\n",
    "    hyps = corpus_ner\n",
    "    refs = []\n",
    "\n",
    "    for index in range(len(corpus_text)):\n",
    "        spacy_doc = extend_entity(corpus_text[index])\n",
    "        tokens = spacy_token(spacy_doc)\n",
    "        refs.append(tokens)\n",
    "\n",
    "    results = conll.evaluate(refs, hyps)\n",
    "    pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "    return pd_tbl.round(decimals=3)\n",
    "\n",
    "accuracy_expansion(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6c6c3-b94a-4113-bdd3-0b78e25f2762",
   "metadata": {},
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1021a0b0-909c-4e1a-bf04-38dbab192fd2",
   "metadata": {},
   "source": [
    "This function returns as output a list containg the string of all the entities readed by SpaCy and its type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adc42fd7-3b71-42e4-b184-6ee95cbef1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AL-AIN', 'ORG'), (',', ''), ('United Arab Emirates', 'ORG'), ('1996-12-06', 'MISC')]\n"
     ]
    }
   ],
   "source": [
    "def add_whitespace(token, string):\n",
    "    if token.whitespace_ == \"\":\n",
    "        string += token.text\n",
    "    else:\n",
    "        string += token.text + \" \"\n",
    "    return string\n",
    "\n",
    "def spacy_to_conll(doc):\n",
    "    conll = []\n",
    "    string = \"\"\n",
    "    ent_type = \"\"\n",
    "    beginI = False\n",
    "    for ind, token in enumerate(doc):\n",
    "        if token.ent_iob_ == 'B':\n",
    "            if beginI:\n",
    "                conll.append((string.rstrip(), ent_type))\n",
    "                string = \"\"\n",
    "                beginI = False\n",
    "\n",
    "            string = add_whitespace(token, string)\n",
    "            ent_type = convert_ent_type(token.ent_type_)\n",
    "\n",
    "            if ind == len(doc)-1:\n",
    "                conll.append((string.rstrip(), ent_type))\n",
    "\n",
    "            beginI = True\n",
    "\n",
    "        elif token.ent_iob_ == 'I':\n",
    "            string = add_whitespace(token, string)\n",
    "            \n",
    "            if ind == len(doc)-1:\n",
    "                conll.append((string.rstrip(), ent_type))\n",
    "\n",
    "        else: # 'O'\n",
    "            if beginI:\n",
    "                conll.append((string.rstrip(), ent_type))\n",
    "                string = \"\"\n",
    "                beginI = False\n",
    "\n",
    "            string = add_whitespace(token, string)\n",
    "\n",
    "            conll.append((string.rstrip(), \"\"))\n",
    "            string = \"\"\n",
    "\n",
    "    return conll\n",
    "\n",
    "print(spacy_to_conll(nlp(corpus_text[2])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "native39",
   "language": "python",
   "name": "native39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
